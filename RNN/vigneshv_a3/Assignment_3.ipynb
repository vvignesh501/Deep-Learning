{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfB17nb-eeHV"
   },
   "outputs": [],
   "source": [
    "## Assignment 3 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnLfPRgNefE7"
   },
   "source": [
    "\n",
    "**Part 1 using RNN with TBTT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "v0C1qYkiCp9w",
    "outputId": "435aaedf-9aee-4d53-ec5b-e3fe98c0960c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108158\n",
      "Vocabulary Size: 11457\n",
      "Total Sequences: 208529\n",
      "208529 208529\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 100)            1145700   \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 500)               300500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11457)             5739957   \n",
      "=================================================================\n",
      "Total params: 7,186,157\n",
      "Trainable params: 7,186,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "208529/208529 [==============================] - 514s 2ms/step - loss: 6.7969 - acc: 0.0460\n",
      "Epoch 2/10\n",
      "208529/208529 [==============================] - 524s 3ms/step - loss: 6.2229 - acc: 0.0799\n",
      "Epoch 3/10\n",
      "208529/208529 [==============================] - 527s 3ms/step - loss: 5.9780 - acc: 0.0944\n",
      "Epoch 4/10\n",
      "208529/208529 [==============================] - 525s 3ms/step - loss: 5.8018 - acc: 0.1021\n",
      "Epoch 5/10\n",
      "208529/208529 [==============================] - 537s 3ms/step - loss: 5.6542 - acc: 0.1091\n",
      "Epoch 6/10\n",
      "208529/208529 [==============================] - 526s 3ms/step - loss: 5.5255 - acc: 0.1148\n",
      "Epoch 7/10\n",
      "208529/208529 [==============================] - 522s 3ms/step - loss: 5.4044 - acc: 0.1186\n",
      "Epoch 8/10\n",
      "208529/208529 [==============================] - 529s 3ms/step - loss: 5.2867 - acc: 0.1217\n",
      "Epoch 9/10\n",
      "208529/208529 [==============================] - 524s 3ms/step - loss: 5.1770 - acc: 0.1235\n",
      "Epoch 10/10\n",
      "208529/208529 [==============================] - 522s 3ms/step - loss: 5.0774 - acc: 0.1240\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open('/content/English Literature.txt', 'r') as f: \n",
    "  sample= f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string1 = format_data(sample)\n",
    "print(len(formatted_string1))\n",
    "\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "formatted_string = regular_exp.tokenize(formatted_string1)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([formatted_string])\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([formatted_string])[0]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# create word -> word sequences\n",
    "sequences = []\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i - 1:i + 1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, 0], sequences[:, 1]\n",
    "\n",
    "print(len(X), len(y))\n",
    "\n",
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "#Model Architecture\n",
    "embedding_size = 100\n",
    "units = 500\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=1))\n",
    "model.add(SimpleRNN(units=units, input_shape=(1, 100), activation='sigmoid'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X, y, epochs=10, verbose=1)\n",
    "model.save('my_simple_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJX4inaHfzng"
   },
   "source": [
    "**Part 2 - BTT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zp4YPV1yf3Nr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, SimpleRNN, GRU, Masking, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open('/content/English Literature.txt', 'r') as f: \n",
    "  sample = f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string = format_data(sample)\n",
    "\n",
    "# integer encode text\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "sent = regular_exp.tokenize(formatted_string)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sent)\n",
    "encoded = tokenizer.texts_to_sequences([sent])[0]\n",
    "\n",
    "sentence_tokenize = nltk.tokenize.sent_tokenize(formatted_string)\n",
    "print(sentence_tokenize)\n",
    "\n",
    "sent_len = 16\n",
    "i = 0\n",
    "input_X = []\n",
    "output_y = []\n",
    "\n",
    "X = []\n",
    "y1 = []\n",
    "padded_sent = []\n",
    "for i in range(len(sentence_tokenize)):\n",
    "    rem_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    sentences = rem_exp.tokenize(sentence_tokenize[i])\n",
    "    encoded = tokenizer.texts_to_sequences([sentences])[0]\n",
    "    if (len(encoded) < sent_len) or (len(encoded) > sent_len):\n",
    "        padded_sent = pad_sequences([encoded], maxlen=sent_len, dtype='int32', padding='pre', truncating='pre',\n",
    "                                    value=0.0)\n",
    "    input_X = padded_sent[0][0:(sent_len - 1)]\n",
    "    output_y = padded_sent[0][1:sent_len]\n",
    "    X.append(input_X)\n",
    "    y1.append(output_y)\n",
    "\n",
    "\n",
    "# integer encode text\n",
    "X = np.array(X)\n",
    "num_y = np.array(y1)\n",
    "print(X, num_y)\n",
    "max_words = 10\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# one hot encode outputs\n",
    "y = to_categorical(num_y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "# define the network architecture: a embedding followed by LSTM\n",
    "embedding_size = 100\n",
    "rnn_size = 50\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_size, input_length=15))\n",
    "model1.add(Masking(mask_value=0.0))\n",
    "model1.add(SimpleRNN(rnn_size, return_sequences=True))\n",
    "model1.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "\n",
    "# fit network\n",
    "model1.fit(X, y, epochs=10, verbose=1, batch_size=1)\n",
    "model1.save(\"my_rnn_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "663bYKpgkDLL"
   },
   "source": [
    "**Explanation for Part 2**\n",
    "\n",
    "\n",
    "**Epoch 1/10**\n",
    "- 211s - loss: 4.5670 - accuracy: 0.3560\n",
    "\n",
    "Epoch 2/10\n",
    " - 218s - loss: 4.1274 - accuracy: 0.3811\n",
    "\n",
    "Epoch 3/10 \n",
    "- 218s - loss: 3.9648 - accuracy: 0.3925\n",
    "\n",
    "Epoch 4/10\n",
    " - 228s - loss: 3.8438 - accuracy: 0.4017\n",
    "\n",
    "Epoch 5/10 \n",
    "- 219s - loss: 3.7456 - accuracy: 0.4108\n",
    "\n",
    "Epoch 6/10\n",
    " - 224s - loss: 3.6586 - accuracy: 0.4193\n",
    "\n",
    "Epoch 7/10 \n",
    "- 217s - loss: 3.5825 - accuracy: 0.4279\n",
    "\n",
    "Epoch 8/10\n",
    " - 215s - loss: 3.5133 - accuracy: 0.4348\n",
    "\n",
    "Epoch 9/10 \n",
    "- 215s - loss: 3.4493 - accuracy: 0.4412\n",
    "\n",
    "**Epoch 10/10**\n",
    " - 216s - loss: 3.3964 - accuracy: 0.4468\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "The document has been split into sentences using nltk.sentence_tokenize function and model has been trained with various minibatches of words in each\n",
    "sentence.\n",
    "\n",
    "There is a gradual decrease in the  loss function using the BTT function compared to TBTT. The accuracy seems to be increasing and better than the first one. However training the model to more epochs can bring more accuracy to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39ZvI24ljTGV"
   },
   "source": [
    "**PART 3.1 - Implementing GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tD05gr7ty8Ws"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, SimpleRNN, GRU, Masking, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open(base_path, encoding='utf-8') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string = format_data(sample)\n",
    "\n",
    "# integer encode text\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "sent = regular_exp.tokenize(formatted_string)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sent)\n",
    "encoded = tokenizer.texts_to_sequences([sent])[0]\n",
    "\n",
    "sentence_tokenize = nltk.tokenize.sent_tokenize(formatted_string)\n",
    "print(sentence_tokenize)\n",
    "\n",
    "sent_len = 16\n",
    "i = 0\n",
    "input_X = []\n",
    "output_y = []\n",
    "\n",
    "X = []\n",
    "y1 = []\n",
    "padded_sent = []\n",
    "for i in range(len(sentence_tokenize)):\n",
    "    rem_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    sentences = rem_exp.tokenize(sentence_tokenize[i])\n",
    "    encoded = tokenizer.texts_to_sequences([sentences])[0]\n",
    "    if (len(encoded) < sent_len) or (len(encoded) > sent_len):\n",
    "        padded_sent = pad_sequences([encoded], maxlen=sent_len, dtype='int32', padding='pre', truncating='pre',\n",
    "                                    value=0.0)\n",
    "    input_X = padded_sent[0][0:(sent_len - 1)]\n",
    "    output_y = padded_sent[0][1:sent_len]\n",
    "    X.append(input_X)\n",
    "    y1.append(output_y)\n",
    "\n",
    "\n",
    "# integer encode text\n",
    "X = np.array(X)\n",
    "num_y = np.array(y1)\n",
    "print(X, num_y)\n",
    "max_words = 10\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# one hot encode outputs\n",
    "y = to_categorical(num_y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "# define the network architecture: a embedding followed by LSTM\n",
    "embedding_size = 100\n",
    "rnn_size = 50\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_size, input_length=15))\n",
    "model1.add(Masking(mask_value=0.0))\n",
    "model1.add(GRU(rnn_size, return_sequences=True))\n",
    "model1.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "\n",
    "# fit network\n",
    "model1.fit(X, y, epochs=10, verbose=1, batch_size=1)\n",
    "model.save('my_gru_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBKLox5OFUve"
   },
   "source": [
    "**Explanation 3.1**\n",
    "\n",
    "**Epoch 1/10**\n",
    "\n",
    "20ms/step - loss: 4.1253 - accuracy: 0.3828\n",
    "\n",
    "263s 21ms/step - loss: 3.9559 - accuracy: 0.3934\n",
    "\n",
    "267s 21ms/step - loss: 3.8300 - accuracy: 0.4033\n",
    "\n",
    "219ms/step - loss: 3.4546 - accuracy: 0.4418\n",
    "\n",
    " 250s 20ms/step - loss: 3.3084 - accuracy: 0.4585\n",
    "\n",
    "245s 20ms/step - loss: 3.2458 - accuracy: 0.4666\n",
    "\n",
    "....\n",
    "....\n",
    "\n",
    "**Epoch 10/10**\n",
    "\n",
    "260s 21ms/step - loss: 3.0573 - accuracy: 0.4887\n",
    "\n",
    "Process finished with exit code 0\n",
    "\n",
    "After comparing the losss with Part 1 and Part 2. We can see that the GRU model performs better than the other two models.\n",
    "\n",
    "As we can see here in the GRU model, the loss is steeply reducing and the accuracy seems to be better than the RNN model. However, increasing the epochs will increase the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UvtKYzGoCkM"
   },
   "source": [
    "**Part 3.2 Experimenting the generated word sequence for all the 3 models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_q1E6XWBo_Ry"
   },
   "source": [
    "Generating the word sequence for Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "DAjkaJ29oqyt",
    "outputId": "630ddf4c-5fdc-4ba9-e81e-1136f7eb98bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The generated word sequences are: [['good lord'], ['first citizen'], ['citizen we'], ['second murderer'], ['talking with'], ['poor soul'], ['bear the'], ['With the'], ['what is'], ['More than']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('punkt')\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open(base_path, encoding='utf-8') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string = format_data(sample)\n",
    "\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "token_sent = regular_exp.tokenize(formatted_string)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_sent)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "\n",
    "def results(model, tokenizer, max_length, input_vector, n_words):\n",
    "    for i in range(len(n_words)):\n",
    "        input_vector = n_words[i]\n",
    "        #print(len(nltk.word_tokenize(input_vector)))\n",
    "        while len(nltk.word_tokenize(input_vector)) <= max_length:\n",
    "            encoded = tokenizer.texts_to_sequences([input_vector])[0]\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, dtype='int32', padding='pre')\n",
    "            predicted_words = model.predict_classes(encoded)\n",
    "            # reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "            out_word = ''\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted_words:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            input_vector = \" \".join((input_vector, out_word))\n",
    "        outputs.append([input_vector])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "n_words = ['good', 'first', 'citizen', 'second', 'talking', 'poor', 'bear', 'With', 'what', 'More']\n",
    "model = load_model('my_simple_model.h5')\n",
    "generated_words = results(model, tokenizer, 1, 'love', n_words)\n",
    "print(\"The generated word sequences are:\", generated_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6qu0RjMOMLm"
   },
   "source": [
    "**All the predicted words after a given input gives the user an understandable next english word.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snC4gMCNpEBM"
   },
   "source": [
    "Generating the word sequernce for Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "1TaTTz7-pHta",
    "outputId": "d7cefca0-8983-4483-e37e-59ca4f66f2d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated RNN word sequences are: [['what is the matter for what is the world that it was which from the crown'], ['first citizen you sir i am not so much on my life s face for this'], ['then be gone and i ll tell you as i am a man of my lord'], ['second citizen i ll tell thee i say i ll be your good night i ll'], ['my horse is this the king and kill d him to the people and yet i'], ['poor boy hence i will not stay me to the king of york and death s'], ['the heavens have done to hand in this way to be thine own with him for'], ['Upon a man of my lord i ll tell you i am not so much on'], ['And what s the matter for t you of me to the tower and with a'], ['but what is not so that i ll tell you all for i am sure of']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open(base_path, encoding='utf-8') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string = format_data(sample)\n",
    "\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "token_sent = regular_exp.tokenize(formatted_string)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_sent)\n",
    "encoded = tokenizer.texts_to_sequences([token_sent])[0]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "\n",
    "def results(model, tokenizer, max_length, input_vector, n_words):\n",
    "    for i in range(len(n_words)):\n",
    "        input_vector = n_words[i]\n",
    "        #print(len(nltk.word_tokenize(input_vector)))\n",
    "        while len(nltk.word_tokenize(input_vector)) <= max_length:\n",
    "            encoded = tokenizer.texts_to_sequences([input_vector])[0]\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, dtype='int32', padding='pre')\n",
    "            predicted_words = model.predict_classes(encoded)\n",
    "            # reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "            out_word = ''\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted_words[0][-1]:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            input_vector = \" \".join((input_vector, out_word))\n",
    "        outputs.append([input_vector])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "n_words = ['what', 'first', 'then', 'second', 'my', 'poor', 'the', 'Upon', 'And', 'but']\n",
    "model = load_model('my_rnn_model.h5', compile=False)\n",
    "generated_words = results(model, tokenizer, 15, 'love', n_words)\n",
    "print(\"The generated RNN word sequences are:\", generated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjZze8ipL3kv"
   },
   "source": [
    "**Ten sentences are generated and the generated RNN word sequence predicts and gives the user some better understandable english words which likely matching to the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7E1b1cZpdF0"
   },
   "source": [
    "Generating the word sequence for Part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "61QQHMSuphJH",
    "outputId": "7e95117e-d1fb-44db-81c0-af0331a6a2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated GRU word sequences are: [['King richard iii what is the matter now sir he is dead and all his name'], ['first musician ay what s a woman of this house of york as thou hast need'], ['Till then be king and now to me and you are at home to be rid'], ['bring forth the way to me and thou shalt know the king and so then to'], ['talking with her with her to prison and welcome home the blood of death hath made'], ['You re welcome home to her in heaven bless thee on the queen s wife for'], ['bear me the king s death shall be so full of you are all the matter'], ['We ll not stay him for the people and that i have heard of them and'], ['when you are here at the least is there is a man that would never were'], ['I am not sir of you and you are enough to you and your daughter sir']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open(base_path, encoding='utf-8') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string = format_data(sample)\n",
    "\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "token_sent = regular_exp.tokenize(formatted_string)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_sent)\n",
    "encoded = tokenizer.texts_to_sequences([token_sent])[0]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "\n",
    "def results(model, tokenizer, max_length, input_vector, n_words):\n",
    "    for i in range(len(n_words)):\n",
    "        input_vector = n_words[i]\n",
    "        #print(len(nltk.word_tokenize(input_vector)))\n",
    "        while len(nltk.word_tokenize(input_vector)) <= 15:\n",
    "            encoded = tokenizer.texts_to_sequences([input_vector])[0]\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, dtype='int32', padding='pre')\n",
    "            predicted_words = model.predict_classes(encoded)\n",
    "            # reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "            out_word = ''\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted_words[0][-1]:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            input_vector = \" \".join((input_vector, out_word))\n",
    "        outputs.append([input_vector])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "n_words = ['King', 'first', 'Till', 'bring', 'talking', 'You', 'bear', 'We', 'when', 'I']\n",
    "model = load_model('my_gru_model.h5', compile=False)\n",
    "generated_words = results(model, tokenizer, 15, 'love', n_words)\n",
    "print(\"The generated GRU word sequences are:\", generated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLVUMmqvp4pm"
   },
   "source": [
    "**Extract a word representation from a trained RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "o51XFcPLp-wN",
    "outputId": "076e9f11-ca35-4242-d4dc-67709997ea63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity for Simple model the two words is 0.7048900127410889:\n",
      "The cosine similarity for RNN model the two words is 0.692296028137207:\n",
      "The cosine similarity for GRU model the two words is 0.569222629070282:\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy import dot\n",
    "\n",
    "base_path = os.path.abspath('English Literature.txt')\n",
    "\n",
    "with open(base_path, encoding='utf-8') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "\n",
    "def format_data(input_string):\n",
    "    clean_string = re.sub(r'\\((\\d+)\\)', r'', input_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\s\\s', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "\n",
    "formatted_string = format_data(sample)\n",
    "\n",
    "# integer encode text\n",
    "regular_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "sent = regular_exp.tokenize(formatted_string)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sent)\n",
    "encoded = tokenizer.texts_to_sequences([sent])[0]\n",
    "\n",
    "model1 = load_model('my_simple_model.h5', compile=False)\n",
    "model2 = load_model('my_rnn_model.h5', compile=False)\n",
    "model3 = load_model('my_gru_model.h5', compile=False)\n",
    "\n",
    "models = [model1, model2, model3]\n",
    "\n",
    "model1_name = 'Simple model'\n",
    "model2_name = 'RNN model'\n",
    "model3_name = 'GRU model'\n",
    "\n",
    "def get_embedding(embeddings):\n",
    "    words_embeddings = {word: embeddings[index] for word, index in tokenizer.word_index.items()}\n",
    "    return words_embeddings\n",
    "\n",
    "\n",
    "def check_similarity(string1, string2):\n",
    "    return dot(string1, string2) / (np.linalg.norm(string1) * np.linalg.norm(string2))\n",
    "\n",
    "\n",
    "values = []\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    embeddings = model.layers[0].get_weights()[0]\n",
    "    list_embedding = get_embedding(embeddings)\n",
    "    string1 = 'have'\n",
    "    string2 = 'had'\n",
    "    a = list_embedding[string1]\n",
    "    b = list_embedding[string2]\n",
    "    word_similarity = check_similarity(a, b)\n",
    "    values.append(word_similarity)\n",
    "\n",
    "print(\"The cosine similarity for {} the two words is {}:\".format(model1_name, values[0]))\n",
    "print(\"The cosine similarity for {} the two words is {}:\".format(model2_name, values[1]))\n",
    "print(\"The cosine similarity for {} the two words is {}:\".format(model3_name, values[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvOA_of11zX6"
   },
   "source": [
    "**Part5- Learning an RNN model that predicts document categories given its content (text classification)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNfvSmAs18FN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "(11797, 200) (11797, 4)\n",
      "(1311, 200) (1311, 4)\n",
      "200\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 11797 samples, validate on 1311 samples\n",
      "Epoch 1/10\n",
      "11797/11797 [==============================] - 69s 6ms/step - loss: 1.3191 - accuracy: 0.3758 - val_loss: 1.2350 - val_accuracy: 0.4348\n",
      "Epoch 2/10\n",
      "11797/11797 [==============================] - 70s 6ms/step - loss: 1.1012 - accuracy: 0.5462 - val_loss: 0.9762 - val_accuracy: 0.6301\n",
      "Epoch 3/10\n",
      "11797/11797 [==============================] - 71s 6ms/step - loss: 0.8613 - accuracy: 0.6791 - val_loss: 0.8788 - val_accuracy: 0.6377\n",
      "Epoch 4/10\n",
      "11797/11797 [==============================] - 67s 6ms/step - loss: 0.6919 - accuracy: 0.7525 - val_loss: 0.8413 - val_accuracy: 0.6598\n",
      "Epoch 5/10\n",
      "11797/11797 [==============================] - 67s 6ms/step - loss: 0.6022 - accuracy: 0.7894 - val_loss: 0.8431 - val_accuracy: 0.6735\n",
      "Epoch 6/10\n",
      "11797/11797 [==============================] - 66s 6ms/step - loss: 0.4707 - accuracy: 0.8389 - val_loss: 0.8531 - val_accuracy: 0.6903\n",
      "Epoch 7/10\n",
      "11797/11797 [==============================] - 68s 6ms/step - loss: 0.3912 - accuracy: 0.8686 - val_loss: 0.8994 - val_accuracy: 0.6888\n",
      "Epoch 8/10\n",
      "11797/11797 [==============================] - 71s 6ms/step - loss: 0.3015 - accuracy: 0.9033 - val_loss: 0.9740 - val_accuracy: 0.6941\n",
      "Epoch 9/10\n",
      "11797/11797 [==============================] - 70s 6ms/step - loss: 0.2452 - accuracy: 0.9214 - val_loss: 1.0315 - val_accuracy: 0.6964\n",
      "Epoch 10/10\n",
      "11797/11797 [==============================] - 70s 6ms/step - loss: 0.2519 - accuracy: 0.9219 - val_loss: 0.9472 - val_accuracy: 0.6934\n",
      "1311/1311 [==============================] - 0s 277us/step\n",
      "Testing Loss: 0.94718\n",
      "  Accuracy: 0.69336\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, SimpleRNN\n",
    "\n",
    "\n",
    "def to_categories(name, cat=[\"politics\", \"rec\", \"comp\", \"religion\"]):\n",
    "    for i in range(len(cat)):\n",
    "        if str.find(name, cat[i]) > -1:\n",
    "            return (i)\n",
    "    print(\"Unexpected folder: \" + name)  # print the folder name which does not include expected categories\n",
    "    return (\"wth\")\n",
    "\n",
    "\n",
    "def data_loader(images_dir):\n",
    "    categories = os.listdir(data_path)\n",
    "    news = []  # news content\n",
    "    groups = []  # category which it belong to\n",
    "\n",
    "    for cat in categories:\n",
    "\n",
    "        # print(\"Category:\" + cat)\n",
    "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
    "            news.append(open(the_new_path, encoding=\"ISO-8859-1\", mode='r').read())\n",
    "            groups.append(cat)\n",
    "\n",
    "    return news, list(map(to_categories, groups))\n",
    "\n",
    "\n",
    "base_path = os.path.abspath('')\n",
    "data_path = os.path.join(base_path, 'PyCharmProjects/RNN/datasets/20news_subsampled/')\n",
    "news, groups = data_loader(data_path)\n",
    "print(news, groups)\n",
    "\n",
    "#tokenized_sents = [nltk.word_tokenize(i) for i in news]\n",
    "\n",
    "max_length = 200\n",
    "embed_size = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news)\n",
    "#word_index = tokenizer.word_index\n",
    "#encoded = tokenizer.texts_to_sequences([news])[0]\n",
    "vocab_size = len(tokenizer.word_index.items()) + 1\n",
    "\n",
    "#word_tokenize = nltk.tokenize.sent_tokenize(news)\n",
    "\n",
    "X1 = []\n",
    "# X = tokenizer.texts_to_sequences([news])[0]\n",
    "for i in range(len(news)):\n",
    "    #tokenizer.fit_on_texts([news[i]])\n",
    "    rem_exp = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    sentences = rem_exp.tokenize(news[i])\n",
    "    encoded = tokenizer.texts_to_sequences([sentences])[0]\n",
    "    X = [pad_sequences([encoded], maxlen=max_length, dtype='int32', padding='pre', truncating='pre', value=0.0)]\n",
    "    X1.append(X)\n",
    "\n",
    "X = np.asarray(X1)\n",
    "X = np.reshape(X, (13108, 200))\n",
    "\n",
    "y = to_categorical(groups, num_classes=4)\n",
    "print(y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.10)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "print(X.shape[1])\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, input_length=200))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, verbose=1, validation_data=(X_test, Y_test))\n",
    "model.save('my_text_classification_model.h5')\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print('Testing Loss: {:0.5f}\\n  Accuracy: {:0.5f}'.format(accuracy[0], accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3F8OAMpIHSf"
   },
   "source": [
    "**Part 5 Explanation**\n",
    "\n",
    "The dataset has been downloaded and divided into training and testing with 90% and 10%.\n",
    "\n",
    "**Report your accuracy results on the validation set**\n",
    "\n",
    "I have trained the model by adding the L2 regularization to avoid the overfitting of the data. By proper splitting of the data and fine tuning the hyperparameters, I have achieved around a **validation accuracy of 78 %** after running 15 epochs. However,increasing the no of epochs further and closely studying the data could improve the validation accuracy much better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Assignment_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
